---
title: "Pipeline for peak coverage"
author: "Briana Mittleman"
date: "7/26/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

I need to create a processing pipeline that I can run each time I get more individuals that will do the following:  

* combine all total and nuclear libraries (as a bigwig/genome coverage)

* call peaks with Yang's script  

* filter peaks with Yang's script  

* clean peaks

* run feature counts on these peaks for all fo the individuals  



##Create bedgraph and bigwig:  

I can do this step in my snakefile. First, I added the following to my environemnt.  

* ucsc-bedgraphtobigwig  
* ucsc-bigwigmerge  
* ucsc-wigtobigwig  
* ucsc-bigwigtobedgraph  

I want to create bedgraph for each file. I will add a rule to my snakefile that does this and puts them in the bedgraph directory.  

```{bash, eval=F}
#add to directory
dir_bedgraph= dir_data + "bedgraph/"

#add to rule_all  

expand(dir_bedgraph + "{samples}.bg", samples=samples)

#rule
rule bedgraph: 
  input:
    bam = dir_sort + "{samples}-sort.bam"
  output: dir_bedgraph + "{samples}.bg"
  shell: "bedtools genomecov -ibam {input.bam} -bg -5 > {output}"
```

I want to add more memory for this rule in the cluster.json  

```{bash, eval=F}
"bedgraph" :
    {
            "mem": 16000
    }
```


I will use the bedgraphtobigwig tool.

```{bash, eval=F}
#add to directory
dir_bigwig= dir_data + "bigwig/"
dir_sortbg= dir_data + "bedgraph_sort/"

#add to rule_all  
expand(dir_sortbg + "{samples}.sort.bg", samples=samples)
expand(dir_bigwig + "{samples}.bw", samples=samples)

rule sort_bg:
    input: dir_bedgraph + "{samples}.bg"
    output: dir_sortbg + "{samples}.sort.bg"
    shell: "sort -k1,1 -k2,2n {input} > {output}"

rule bg_to_bw:
    input: 
        bg=dir_sortbg + "{samples}.sort.bg"
        len= chrom_length 
    output: dir_bigwig + "{samples}.bw"
    shell: "bedGraphToBigWig {input.bg} {input.len} {output}"
```

##Merge BW

This next step will take all of the files in the bigwig directory and merge them. To do this I will create a script that creates a list of all of the files then uses this list in the merge script.

mergeBW.sh  

```{bash, eval=F}
#!/bin/bash

#SBATCH --job-name=mergeBW
#SBATCH --account=pi-yangili1
#SBATCH --time=24:00:00
#SBATCH --output=mergeBW.out
#SBATCH --error=mergeBW.err
#SBATCH --partition=broadwl
#SBATCH --mem=40G
#SBATCH --mail-type=END

module load Anaconda3
source activate three-prime-env

ls -d -1 /project2/gilad/briana/threeprimeseq/data/bigwig/* | tail -n +2 > /project2/gilad/briana/threeprimeseq/data/list_bw/list_of_bigwig.txt

bigWigMerge -inList /project2/gilad/briana/threeprimeseq/data/list_bw/list_of_bigwig.txt /project2/gilad/briana/threeprimeseq/data/mergedBW/merged_combined_YL-SP-threeprimeseq.bg

```

The result of this script will be a merged bedgraph of all of the files.  

##Convert to coverage 
```{r}
library(workflowr)
library(ggplot2)
library(dplyr)
```
 

```{bash, eval=F}
#!/usr/bin/env python


main(inFile, outFile):
    fout = open(outFile,'w')
    for ind,ln in enumerate(open(inFile)):
      print(ind)
      chrom, start, end, count = ln.split()
      i2=int(start)
      while i2 < int(end):
        fout.write("%s\t%d\t%s\n"%(chrom, i2 + 1, count))
        fout.flush()
        i2 += 1
    fout.close()    
    

if __name__ == "__main__":
    import numpy as np
    from misc_helper import *
    import sys
    inFile = sys.argv[1]
    outFile = sys.argv[2]
    main(inFile, outFile)
```

Create a bash script to run this.  I want the input and output files to be arguments in the python script.  

```{bash, eval=F}
#!/bin/bash

#SBATCH --job-name=run_bgtocov
#SBATCH --account=pi-yangili1
#SBATCH --time=24:00:00
#SBATCH --output=run_bgtocov.out
#SBATCH --error=run_bgtocov.err
#SBATCH --partition=broadwl
#SBATCH --mem=12G
#SBATCH --mail-type=END

module load Anaconda3
source activate three-prime-env 

python bg_to_cov.py "/project2/gilad/briana/threeprimeseq/data/mergedBW/merged_combined_YL-SP-threeprimeseq.bg" "/project2/gilad/briana/threeprimeseq/data/mergedBW/merged_combined_YL-SP-threeprimeseq.coverage.txt"
```


##Change to genome coverage  (NEW SCRIPT DONT NEED THIS)   

Add zeros to the bedgraph to make it a genome coverage file.  

```{bash, eval=F}
awk '{print $1 "\t" $2 "\t" "0"}' /project2/gilad/briana/threeprimeseq/data/bedgraph_comb/NuclearBamFiles.split.genomecov.bed > /project2/gilad/briana/threeprimeseq/data/mergedBW/genomecov_zero.txt 
```

Try this with bash:  

```{bash, eval=F}
#!/bin/bash

#SBATCH --job-name=addzero_bash
#SBATCH --account=pi-yangili1
#SBATCH --time=24:00:00
#SBATCH --output=addzerobash.out
#SBATCH --error=addzerobash.err
#SBATCH --partition=broadwl
#SBATCH --mem=12G
#SBATCH --mail-type=END

module load Anaconda3
source activate three-prime-env

sort -k1,1 -k2,2n /project2/gilad/briana/threeprimeseq/data/mergedBW/merged_combined_YL-SP-threeprimeseq.coverage.txt > /project2/gilad/briana/threeprimeseq/data/mergedBW/merged_combined_YL-SP-threeprimeseq.coverage.sort.txt

less /project2/gilad/briana/threeprimeseq/data/mergedBW/merged_combined_YL-SP-threeprimeseq.coverage.sort.txt | awk '{print($1"^"$2"\t"$3)}'  >  /project2/gilad/briana/threeprimeseq/data/mergedBW/temp1.txt

less /project2/gilad/briana/threeprimeseq/data/mergedBW/genomecov_zero.txt | awk '{print($1"^"$2"\t"$3)}' >  /project2/gilad/briana/threeprimeseq/data/mergedBW/temp2.txt

join  -a1  -a2 -o '0,1.2' -e 0  /project2/gilad/briana/threeprimeseq/data/mergedBW/temp1.txt /project2/gilad/briana/threeprimeseq/data/mergedBW/temp2.txt | tr '^' '\t' | tr ' ' '\t' | cut -f1-4 > /project2/gilad/briana/threeprimeseq/data/mergedBW/merged_combined_YL-SP-threeprimeseq.coverage.sort.with0.bash.txt

```

sort_gencov.sh  

```{bash, eval=F}
#!/bin/bash

#SBATCH --job-name=sort_gencov
#SBATCH --account=pi-yangili1
#SBATCH --time=24:00:00
#SBATCH --output=sortgencov.out
#SBATCH --error=sortgencov.err
#SBATCH --partition=bigmem2
#SBATCH --mem=200G
#SBATCH --mail-type=END


sort -k1,1 -k2,2n /project2/gilad/briana/threeprimeseq/data/mergedBW/merged_combined_YL-SP-threeprimeseq.coverage.sort.with0.bash.txt > /project2/gilad/briana/threeprimeseq/data/mergedBW/merged_combined_YL-SP-threeprimeseq.coverage.sort.with0.bash.sort.txt
```

##Call Peaks 


Run yangs scrip on /project2/gilad/briana/threeprimeseq/data/mergedBW/merged_combined_YL-SP-threeprimeseq.coverage.sort.with0.sort.txt by making this the input file in the  callPeaksYL_GEN.py   

*  inFile = "/project2/gilad/briana/threeprimeseq/data/mergedBW/merged_combined_YL-SP-threeprimeseq.coverage.sort.with0.bash.sort.txt"  



```{bash, eval=F}
#!/bin/bash

#SBATCH --job-name=w_getpeakYLgen
#SBATCH --account=pi-yangili1
#SBATCH --time=24:00:00
#SBATCH --output=w_getpeakYLgen.out
#SBATCH --error=w_getpeakYLgen.err
#SBATCH --partition=broadwl
#SBATCH --mem=12G
#SBATCH --mail-type=END

module load Anaconda3
source activate three-prime-env


for i in $(seq 1 22); do 
  python callPeaksYL_GEN.py $i
done
```



Run the file with : sbatch w_getpeakYLGEN.sh 

After I have the peaks I will need to use Yangs filter peak function.  




##Problem with peak script : try with bam merge  

```{bash, eval=F}
#!/bin/bash

#SBATCH --job-name=comb_gencov
#SBATCH --account=pi-yangili1
#SBATCH --time=24:00:00
#SBATCH --output=comb_gencov.out
#SBATCH --error=comb_gencov.err
#SBATCH --partition=bigmem2
#SBATCH --mem=100G
#SBATCH --mail-type=END


module load Anaconda3
source activate three-prime-env 


samtools merge /project2/gilad/briana/threeprimeseq/data/comb_bam/all_total.nuc_comb.bam  /project2/gilad/briana/threeprimeseq/data/sort/*.bam


bedtools genomecov -ibam /project2/gilad/briana/threeprimeseq/data/comb_bam/all_total.nuc_comb.bam -d -split > /project2/gilad/briana/threeprimeseq/data/comb_bam/all_total.nuc_comb.split.genomecov.bed
```





##Filter peaks    
Update each of the following scripts:    


1. Combine the peaks from all of the chromosome peak files. 

```{bash, eval=F}
cat /project2/gilad/briana/threeprimeseq/data/mergedPeaks/*.bed > 
```



bed2saf.py

* input: peaks bed  file   
* output: peaks saf file   
  
run_feature.sh  

* featureCounts -a PEAK.saf -F SAF -o APAquant.fc /project2/gilad/briana/threeprimeseq/data/sort/*-sort.bam -s 1  



filter_peaks.py  

* input: APAquant.fc  
* output: filtered peaks  


##Clean peaks  


##Ind. Coverage with feature counts  


#Update:

Add a rule to the snakefile that creates the 5' bp resolution but also one that uses the whole read. I will use the whole read method for peak calling.  


```{bash, eval=F}
#add to directory
dir_bedgraph= dir_data + "bedgraph/"
dir_bigwig= dir_data + "bigwig/"
dir_sortbg= dir_data + "bedgraph_sort/"
dir_bedgraph_5= dir_data + "bedgraph_5prime/"

#add to rule_all  

expand(dir_bedgraph + "{samples}.split.bg", samples=samples)
expand(dir_sortbg + "{samples}.sort.bg", samples=samples)
expand(dir_bigwig + "{samples}.bw", samples=samples)
expand(dir_bedgraph_5 + "{samples}.5.bg", samples=samples)

#rule
rule bedgraph_5: 
  input:
    bam = dir_sort + "{samples}-sort.bam"
  output: dir_bedgraph_5 + "{samples}.5.bg"
  shell: "bedtools genomecov -ibam {input.bam} -bg -5 > {output}"
  
rule bedgraph: 
  input:
    bam = dir_sort + "{samples}-sort.bam"
  output: dir_bedgraph + "{samples}.split.bg"
  shell: "bedtools genomecov -ibam {input.bam} -bg -split > {output}"

rule sort_bg:
    input: dir_bedgraph + "{samples}.split.bg"
    output: dir_sortbg + "{samples}.sort.bg"
    shell: "sort -k1,1 -k2,2n {input} > {output}"

rule bg_to_bw:
    input: 
        bg=dir_sortbg + "{samples}.sort.bg"
        len= chrom_length 
    output: dir_bigwig + "{samples}.bw"
    shell: "bedGraphToBigWig {input.bg} {input.len} {output}"
```


Will need to run mergeBW.sh and run_bgtocov.sh then call peaks with the updated callpeaks script from yang  (get_APA_peaks.py)  I run this with w_getpeakYLGEN.sh. 


